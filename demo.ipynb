{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 20\n",
      "metadata to be hide:  0\n",
      "ResNet model type: ResNet8\n",
      "User/resource metadata after meta data removal: 16\n",
      "User/resource metadata after meta data removal: 16\n",
      "shape of x_train after encoding (8772, 16)\n",
      "shape of x_test after encoding (2192, 16)\n",
      "batch size: 16\n",
      "shape of x_train after adding new dimension (8772, 16)\n",
      "shape of x_test after adding new dimension (2192, 16)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import sys\n",
    "\n",
    "debug = True\n",
    "\n",
    "# make a function to preprocess dataset\n",
    "trainDataFileName = 'dataset/synthetic/u4k-r4k-auth11k/train_u4k-r4k-auth11k.sample'\n",
    "testDataFileName = 'dataset/synthetic/u4k-r4k-auth11k/test_u4k-r4k-auth11k.sample'\n",
    "\n",
    "batch_size = 16  # trained all networks with batch_size=16\n",
    "\n",
    "# format of the dataset\n",
    "# <uid rid> <8-13 user-metadata values> <8-13 resource-metadata values> <4 operations>\n",
    "# load the train dataset\n",
    "raw_train_dataset = loadtxt(trainDataFileName, delimiter=' ', dtype=str)\n",
    "cols = raw_train_dataset.shape[1]\n",
    "train_dataset = raw_train_dataset[:,2:cols] # TO SKIP UID RID\n",
    "\n",
    "# load the test dataset\n",
    "raw_test_dataset = loadtxt(testDataFileName, delimiter=' ', dtype=str)\n",
    "test_dataset = raw_test_dataset[:,2:cols] # TO SKIP UID RID\n",
    "\n",
    "# columns after removing uid/rid\n",
    "cols = train_dataset.shape[1]\n",
    "if debug:\n",
    "  print('Total columns:', cols)\n",
    "\n",
    "# determine number of metadata to be hide\n",
    "# we will expose first eight user and first eight resource metadata to the model\n",
    "# there are four operations\n",
    "# 8 + 8 + 4 = 20\n",
    "\n",
    "if cols > 20:\n",
    "    hide_meta_data = cols - 20\n",
    "else:\n",
    "    hide_meta_data = 0\n",
    "print('metadata to be hide: ', hide_meta_data)\n",
    "\n",
    "# Compute depth and number of epochs based on metadata hide\n",
    "# We use more deeper network for the dataset where metadata needs to hide\n",
    "# If the dataset needs to hide metadata, then the depth of network is 56, otherwise 8\n",
    "# The value of n helps to determine the depth of network\n",
    "if hide_meta_data > 0:\n",
    "    n = 9\n",
    "else:\n",
    "    n = 1\n",
    "\n",
    "depth = n * 6 + 2\n",
    "\n",
    "# we need less epoch for the deeper network\n",
    "if depth > 8:\n",
    "  epochs = 30\n",
    "else:\n",
    "  epochs = 60\n",
    "\n",
    "# Model name, depth and version\n",
    "model_type = 'ResNet%d' % (depth)\n",
    "if debug:\n",
    "  print('ResNet model type:', model_type)\n",
    "\n",
    "\n",
    "# number of metadata\n",
    "metadata = cols - 4\n",
    "\n",
    "umeta_end = 8\n",
    "rmeta_end = 16\n",
    "umeta_hide_end = umeta_end + hide_meta_data\n",
    "rmeta_hide_end = rmeta_end + hide_meta_data\n",
    "\n",
    "# split x, y from train dataset\n",
    "x_train = train_dataset[:,0:metadata]\n",
    "y_train = train_dataset[:,metadata:cols].astype(int)\n",
    "\n",
    "# hide (remove) user metadata after first eight metadata\n",
    "x_train = np.delete(x_train, slice(umeta_end, umeta_hide_end), 1)\n",
    "# hide (remove) resource metadata after first eight resource metadata\n",
    "x_train = np.delete(x_train, slice(rmeta_end, rmeta_hide_end), 1)\n",
    "if debug:\n",
    "  print('User/resource metadata after meta data removal:', x_train.shape[1])\n",
    "\n",
    "# split x, y from test dataset\n",
    "x_test = test_dataset[:,0:metadata]\n",
    "y_test = test_dataset[:,metadata:cols].astype(int)\n",
    "\n",
    "# hide (remove) user/resource metadata after first eight of user/resource metadata\n",
    "x_test = np.delete(x_test, slice(umeta_end, umeta_hide_end), 1)\n",
    "x_test = np.delete(x_test, slice(rmeta_end, rmeta_hide_end), 1)\n",
    "if debug:\n",
    "  print('User/resource metadata after meta data removal:', x_test.shape[1])\n",
    "\n",
    "############### OneHot ENCODING ##############\n",
    "#x_train = to_categorical(x_train)\n",
    "#x_test = to_categorical(x_test)\n",
    "\n",
    "if debug:\n",
    "  print('shape of x_train after encoding', x_train.shape)\n",
    "  print('shape of x_test after encoding', x_test.shape)\n",
    "#######################################\n",
    "\n",
    "#determine batch size\n",
    "batch_size = min(x_train.shape[0]/10, batch_size)\n",
    "if debug:\n",
    "  print('batch size: ' + str(batch_size))\n",
    "\n",
    "# adding an extra dimension to make the input appropriate for ResNet\n",
    "#x_train = x_train[..., np.newaxis]\n",
    "#x_test = x_test[..., np.newaxis]\n",
    "\n",
    "if debug:\n",
    "  print('shape of x_train after adding new dimension', x_train.shape)\n",
    "  print('shape of x_test after adding new dimension', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]\n",
      " [0 0 0 1]\n",
      " [1 0 1 1]\n",
      " ...\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# svm classifirer\n",
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='rbf') # Linear Kernel\n",
    "print(y_train)\n",
    "y_train_new = [int(\"\".join(str(x) for x in y), 2) for y in y_train]\n",
    "y_test_new = [int(\"\".join(str(x) for x in y), 2) for y in y_test]\n",
    "#Train the model using the training sets\n",
    "clf.fit(x_train, y_train_new)\n",
    "\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_new, y_pred))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule_resnet8(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 59:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 39:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 19:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "def lr_schedule_resnet8_up(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 29:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 19:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 9:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=4):\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "\n",
    "    full = GlobalAveragePooling2D()(x)\n",
    "    out = Dense(num_classes, activation='sigmoid', kernel_initializer='he_normal')(full)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "# make a function to store resnet to generate models for multiple data files\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "dlbac_alpha = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "if depth > 8:\n",
    "  dlbac_alpha.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr_schedule_resnet8_up(0)),\n",
    "              metrics=['binary_accuracy'])\n",
    "  lr_scheduler = LearningRateScheduler(lr_schedule_resnet8_up)\n",
    "else:\n",
    "  dlbac_alpha.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr_schedule_resnet8(0)),\n",
    "              metrics=['binary_accuracy'])\n",
    "  lr_scheduler = LearningRateScheduler(lr_schedule_resnet8)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [lr_reducer, lr_scheduler]\n",
    "\n",
    "outputFileName = 'dlbac_alpha-R'\n",
    "DIR_ASSETS = '02-trained/'\n",
    "PATH_MODEL = DIR_ASSETS + outputFileName + '.hdf5'\n",
    "\n",
    "history = dlbac_alpha.fit(x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks)\n",
    "\n",
    "if debug:\n",
    "  print('Saving trained dlbac_alpha to {}.'.format(PATH_MODEL))\n",
    "if not os.path.isdir(DIR_ASSETS):\n",
    "    os.mkdir(DIR_ASSETS)\n",
    "dlbac_alpha.save(PATH_MODEL)\n",
    "\n",
    "#save history to separate file\n",
    "import pickle\n",
    "\n",
    "PATH_HISTORY_FILE = DIR_ASSETS + 'history_' + outputFileName\n",
    "with open(PATH_HISTORY_FILE, 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test from saved model\n",
    "RESULT_FILE = DIR_ASSETS + 'result.txt'\n",
    "result_file = open(RESULT_FILE, 'w+')\n",
    "result_file.write('train data file name:%s\\n' % (trainDataFileName))\n",
    "result_file.write('test data file name:%s\\n' % (testDataFileName))\n",
    "\n",
    "scores = dlbac_alpha.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "'ResNet%d' % (depth)\n",
    "result_file.write('Test loss:%f\\n' % (scores[0]))\n",
    "print('Test accuracy:', scores[1])\n",
    "result_file.write('Test accuracy:%f\\n' % (scores[1]))\n",
    "\n",
    "# measure True Positive/ Negative, False Positive/ Negative\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, confusion_matrix\n",
    "\n",
    "y_preds = dlbac_alpha.predict(x_test)\n",
    "y_preds = (y_preds > 0.5).astype(int)\n",
    "\n",
    "g_tn = 0\n",
    "g_fp = 0\n",
    "g_fn = 0\n",
    "g_tp = 0\n",
    "\n",
    "# Measure True Positive/ Negative, False Positive/ Negative for each operation, \n",
    "# then combine it to measure actual counts\n",
    "# we calculate the FPR, FNR offline\n",
    "print('True Positive/ Negative, False Positive/ Negative Information')\n",
    "for i in range(4):\n",
    "  tn, fp, fn, tp = confusion_matrix(y_test[:, i:i+1], y_preds[:, i:i+1]).ravel()\n",
    "  print('op%d  # tn: %s, fp: %s, fn: %s, tp: %s' % (i+1, tn, fp, fn, tp))\n",
    "  g_tn = g_tn + tn\n",
    "  g_fp = g_fp + fp\n",
    "  g_fn = g_fn + fn\n",
    "  g_tp = g_tp + tp\n",
    "print('All operations # tn: %s, fp: %s, fn: %s, tp: %s' % (g_tn, g_fp, g_fn, g_tp))\n",
    "result_file.write('TN: %s, FP: %s, FN: %s, TP: %s' % (g_tn, g_fp, g_fn, g_tp))\n",
    "result_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1463c45675535f367d6fe7cdd9900efb7d2f2211cff7dab38e850dfea141f09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
